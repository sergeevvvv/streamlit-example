{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b2a683-9a01-4aa2-b3db-867e27a70ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:58:23.260587Z",
     "iopub.status.busy": "2022-06-28T10:58:23.260286Z",
     "iopub.status.idle": "2022-06-28T10:59:25.379464Z",
     "shell.execute_reply": "2022-06-28T10:59:25.378790Z",
     "shell.execute_reply.started": "2022-06-28T10:58:23.260510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtext\n",
      "  Downloading torchtext-0.12.0-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.19.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.26.8)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa18ed12-9a1d-487c-93ba-7ea42642ae46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:25.381552Z",
     "iopub.status.busy": "2022-06-28T10:59:25.381369Z",
     "iopub.status.idle": "2022-06-28T10:59:27.548568Z",
     "shell.execute_reply": "2022-06-28T10:59:27.547903Z",
     "shell.execute_reply.started": "2022-06-28T10:59:25.381528Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install sympy\n",
    "\n",
    "import math\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt                                                 \n",
    "#import sympy\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed1a4ef-7721-4974-8786-07a61bfee50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.550284Z",
     "iopub.status.busy": "2022-06-28T10:59:27.549742Z",
     "iopub.status.idle": "2022-06-28T10:59:27.694597Z",
     "shell.execute_reply": "2022-06-28T10:59:27.693910Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.550253Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('dataset.csv')[['OUTPUT:result', 'latex']]\n",
    "df2 = pd.read_csv('dataset2.csv')[['OUTPUT:result', 'latex']]\n",
    "df3 = pd.read_csv('dataset3.csv')[['rus', 'latex']]\n",
    "df1 = df1.rename(columns={'OUTPUT:result':'rus', 'latex': 'latex'})\n",
    "df2 = df2.rename(columns={'OUTPUT:result':'rus', 'latex': 'latex'})\n",
    "df = pd.concat([df1,df3],axis=0)\n",
    "train = df\n",
    "#\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#train, test = train_test_split(df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b792af81-f1c5-426f-84d7-51c77492f2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.696334Z",
     "iopub.status.busy": "2022-06-28T10:59:27.696149Z",
     "iopub.status.idle": "2022-06-28T10:59:27.700063Z",
     "shell.execute_reply": "2022-06-28T10:59:27.699577Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.696314Z"
    }
   },
   "outputs": [],
   "source": [
    "trainen = train['latex'].values.tolist()\n",
    "trainja = train['rus'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12843272-0e08-4e50-a699-00fcb0481826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.701035Z",
     "iopub.status.busy": "2022-06-28T10:59:27.700871Z",
     "iopub.status.idle": "2022-06-28T10:59:27.708121Z",
     "shell.execute_reply": "2022-06-28T10:59:27.707679Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.701017Z"
    }
   },
   "outputs": [],
   "source": [
    "def pattern_search(s, pattern):\n",
    "    k = 0\n",
    "    i = 0\n",
    "    shift_start = False\n",
    "    res = ''\n",
    "    buffer = ''\n",
    "    open_bracket = False\n",
    "    close_bracket = False\n",
    "    \n",
    "    while i < len(s):\n",
    "        #print(s[i])\n",
    "        if close_bracket and s[i]=='{':\n",
    "            clost_bracket_index += 1\n",
    "        elif close_bracket and s[i]=='}':\n",
    "            close_bracket = False\n",
    "        elif open_bracket and s[i]=='{':\n",
    "            open_bracket = False\n",
    "            clost_bracket_index = 0\n",
    "            close_bracket = True\n",
    "        else:\n",
    "            \n",
    "            if s[i]==pattern[k]:\n",
    "                k+=1\n",
    "                shift_start = True\n",
    "                buffer += s[i]\n",
    "                if k >= len(pattern):\n",
    "                    shift_start = False\n",
    "                    k = 0\n",
    "                    buffer = ''\n",
    "                    open_bracket = True\n",
    "            else:\n",
    "                shift_start = False\n",
    "                k = 0\n",
    "                res += buffer\n",
    "                buffer = ''\n",
    "                res += s[i]\n",
    "        i+=1\n",
    "    return res\n",
    "\n",
    "def clear_latex(s):\n",
    "    ar = [\n",
    "        r'\\mathnormal',\n",
    "        r'\\mathrm',\n",
    "        r'\\mathit',\n",
    "        r'\\mathbf',\n",
    "        r'\\mathsf',\n",
    "        r'\\mathtt',\n",
    "        r'\\mathbb',\n",
    "        r'\\mathfrak',\n",
    "        r'\\mathcal'\n",
    "    ]\n",
    "    \n",
    "    for pattern in ar:\n",
    "        s = pattern_search(s, pattern)\n",
    "    \n",
    "    s = s.replace('\\\\times','')\n",
    "    return s\n",
    "\n",
    "#s = r'\\mathbb {A} _{A}(P,\\alpha )^{\\times }.'\n",
    "#clear_latex(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc6b35a-595d-49f4-a215-733d17bdbea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.709582Z",
     "iopub.status.busy": "2022-06-28T10:59:27.709327Z",
     "iopub.status.idle": "2022-06-28T10:59:27.720013Z",
     "shell.execute_reply": "2022-06-28T10:59:27.719514Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.709562Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# bug - when can parse prefix correct, other chars from string with prefix became solo-chars\n",
    "import matplotlib\n",
    "\n",
    "def valid(buffer):\n",
    "    if buffer == '\\\\': return False\n",
    "    fig = plt.figure() \n",
    "    try:\n",
    "        plt.text(0.03, 0.3, r\"$%s$\" % buffer)                                  \n",
    "    except Exception as e:\n",
    "        return False\n",
    "    try: fig.savefig(\"save_file_name.pdf\")\n",
    "    except Exception as e: return False\n",
    "\n",
    "    try: plt.close('all')\n",
    "    except Exception as e: return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def latex_split(eqs):\n",
    "    syms = []\n",
    "    buffer = ''\n",
    "    \n",
    "    for eq in eqs.split(' '):\n",
    "        for i in eq:\n",
    "            buffer += i\n",
    "            if valid(buffer):\n",
    "                syms += [buffer]\n",
    "                buffer = ''\n",
    "    return syms\n",
    "            \n",
    "#DF_SPLITTED = []\n",
    "#for exp in latexs[:1]:\n",
    "#    res = latex_split(exp)\n",
    "#    DF_SPLITTED += [res]\n",
    "#print(DF_SPLITTED)\n",
    "#plt.close()\n",
    "\n",
    "def simple_latex_splitter(t_raw):\n",
    "    #t_raw = '\\\\gcd(a,b)=\\\\prod _{p}p^{\\\\min(a_{p},b_{p})}'\n",
    "    res = []\n",
    "    buffer = ''\n",
    "    \n",
    "    command = False\n",
    "    for char in t_raw:\n",
    "        if char in ['{','(','[']:\n",
    "            res += [buffer]\n",
    "            buffer = ''\n",
    "            command = True\n",
    "        if command:\n",
    "            buffer += char\n",
    "        else:\n",
    "            buffer += char\n",
    "        if char in ['}',')',']']:\n",
    "            res += [buffer]\n",
    "            buffer = ''\n",
    "            command = False\n",
    "    res += [buffer]\n",
    "    buffer = ''\n",
    "    return res\n",
    "\n",
    "#def simple_latex_splitter(t_raw):\n",
    "#    #t_raw = '\\\\gcd(a,b)=\\\\prod _{p}p^{\\\\min(a_{p},b_{p})}'\n",
    "#    res = []\n",
    "#    buffer = ''\n",
    "#    \n",
    "#    command = False\n",
    "#    for char in t_raw:\n",
    "#        if char in ['{','(','[']:\n",
    "#            #res += [char]\n",
    "#            res += [buffer]\n",
    "#            buffer = ''\n",
    "#            command = True\n",
    "#            \n",
    "#        if char in ['}',')',']']:\n",
    "#            res += [buffer]\n",
    "#            #res += [char]\n",
    "#            buffer = ''\n",
    "#            command = False\n",
    "#            \n",
    "#        if command:\n",
    "#            buffer += char\n",
    "#        \n",
    "#    res += [buffer]\n",
    "#    buffer = ''\n",
    "#    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f0cb09-72b1-4218-ac5f-e1884c0ea071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.720994Z",
     "iopub.status.busy": "2022-06-28T10:59:27.720834Z",
     "iopub.status.idle": "2022-06-28T10:59:27.725943Z",
     "shell.execute_reply": "2022-06-28T10:59:27.725477Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.720974Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSplitter:\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        \n",
    "    def encode_rus(self, sentence):\n",
    "        return sentence.split(' ')\n",
    "    \n",
    "    def encode_latex(self, sentence):\n",
    "        sentence = clear_latex(sentence)\n",
    "        #sentence = latex_split(sentence)\n",
    "        sentence = simple_latex_splitter(sentence)\n",
    "        plt.close()\n",
    "        plt.close('all')\n",
    "        return sentence\n",
    "    \n",
    "    def encode(self, sentence, out_type):\n",
    "        if self.lang == 'rus':\n",
    "            return self.encode_rus(sentence)\n",
    "        else: return self.encode_latex(sentence)\n",
    "\n",
    "en_tokenizer = CustomSplitter('latex')\n",
    "ja_tokenizer = CustomSplitter('rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9443cda-b00c-4562-b411-ffca77fa928c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:32:04.950355Z",
     "iopub.status.busy": "2022-06-28T10:32:04.950090Z",
     "iopub.status.idle": "2022-06-28T10:32:04.960948Z",
     "shell.execute_reply": "2022-06-28T10:32:04.959595Z",
     "shell.execute_reply.started": "2022-06-28T10:32:04.950328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' A _', '{A}', '', '(P,\\\\alpha )', '^', '{ }', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.encode('\\\\mathbb {A} _{A}(P,\\\\alpha )^{\\\\times }.', out_type='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d4f87f-fb0a-4d1a-b847-3ed17c742b10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:32:05.819625Z",
     "iopub.status.busy": "2022-06-28T10:32:05.818840Z",
     "iopub.status.idle": "2022-06-28T10:32:05.826158Z",
     "shell.execute_reply": "2022-06-28T10:32:05.825369Z",
     "shell.execute_reply.started": "2022-06-28T10:32:05.819594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['年金', '日本に住んでいる20歳~60歳の全ての人は、公的年金制度に加入しなければなりません。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_tokenizer.encode(\"年金 日本に住んでいる20歳~60歳の全ての人は、公的年金制度に加入しなければなりません。\", out_type='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca8f4c4-cada-4d04-992b-5d370dfc05b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.726849Z",
     "iopub.status.busy": "2022-06-28T10:59:27.726688Z",
     "iopub.status.idle": "2022-06-28T10:59:27.990866Z",
     "shell.execute_reply": "2022-06-28T10:59:27.990152Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.726825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2444/2444 [00:00<00:00, 269426.73it/s]\n",
      "100%|██████████| 2444/2444 [00:00<00:00, 33498.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_vocab(sentences, tokenizer):\n",
    "    counter = Counter()\n",
    "    for sentence in tqdm(sentences):\n",
    "        counter.update(tokenizer.encode(sentence, out_type=str))\n",
    "        plt.close('all')\n",
    "    return torchtext.vocab.vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "ja_vocab = build_vocab(trainja, ja_tokenizer)\n",
    "en_vocab = build_vocab(trainen, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f970f693-a4da-4111-b602-5013b79558f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:27.992021Z",
     "iopub.status.busy": "2022-06-28T10:59:27.991849Z",
     "iopub.status.idle": "2022-06-28T10:59:28.109998Z",
     "shell.execute_reply": "2022-06-28T10:59:28.109293Z",
     "shell.execute_reply.started": "2022-06-28T10:59:27.992000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2444it [00:00, 22364.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_process(ja, en):\n",
    "    data = []\n",
    "    for (raw_ja, raw_en) in tqdm(zip(ja, en)):\n",
    "        ja_tensor_ = torch.tensor([ja_vocab[token] for token in ja_tokenizer.encode(raw_ja.rstrip(\"\\n\"), out_type=str)],\n",
    "                            dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer.encode(raw_en.rstrip(\"\\n\"), out_type=str)],\n",
    "                            dtype=torch.long)\n",
    "        data.append((ja_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "ja_vocab.set_default_index(0) # maybe not unknown\n",
    "en_vocab.set_default_index(0)\n",
    "\n",
    "train_data = data_process(trainja, trainen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055e831a-1e8d-4bfb-b67a-7d6858e813a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:38:59.523020Z",
     "iopub.status.busy": "2022-06-28T08:38:59.522834Z",
     "iopub.status.idle": "2022-06-28T08:38:59.527154Z",
     "shell.execute_reply": "2022-06-28T08:38:59.526595Z",
     "shell.execute_reply.started": "2022-06-28T08:38:59.522998Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#with open('train_data2.pickle', 'wb') as handle:\n",
    "#    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#    \n",
    "#with open('ja_vocab2.pickle', 'wb') as handle:\n",
    "#    pickle.dump(ja_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#    \n",
    "#with open('en_vocab2.pickle', 'wb') as handle:\n",
    "#    pickle.dump(en_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "#with open('train_data.pickle', 'rb') as handle:\n",
    "#    train_data_pickle = pickle.load(handle)\n",
    "#with open('ja_vocab.pickle', 'rb') as handle:\n",
    "#    ja_vocab_pickle = pickle.load(handle)\n",
    "#with open('en_vocab.pickle', 'rb') as handle:\n",
    "#    en_vocab_pickle = pickle.load(handle)\n",
    "#\n",
    "#train_data_f = train_data\n",
    "#ja_vocab_f = ja_vocab\n",
    "#en_vocab_f = en_vocab\n",
    "#\n",
    "#train_data = train_data_f + train_data_pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_data_pickle\n",
    "\n",
    "#from tqdm import tqdm\n",
    "#\n",
    "#def build_vocab(sentences, tokenizer):\n",
    "#    counter = Counter()\n",
    "#    for sentence in tqdm(sentences):\n",
    "#        counter.update(tokenizer.encode(sentence, out_type=str))\n",
    "#        plt.close('all')\n",
    "#    return torchtext.vocab.vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "#ja_vocab = build_vocab(trainja[:10], ja_tokenizer)\n",
    "#en_vocab = build_vocab(trainen[:10], en_tokenizer)\n",
    "#\n",
    "#def data_process(ja, en):\n",
    "#    data = []\n",
    "#    for (raw_ja, raw_en) in zip(ja, en):\n",
    "#        ja_tensor_ = torch.tensor([ja_vocab[token] for token in ja_tokenizer.encode(raw_ja.rstrip(\"\\n\"), out_type=str)],\n",
    "#                            dtype=torch.long)\n",
    "#        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer.encode(raw_en.rstrip(\"\\n\"), out_type=str)],\n",
    "#                            dtype=torch.long)\n",
    "#        data.append((ja_tensor_, en_tensor_))\n",
    "#    return data\n",
    "#train_data = data_process(trainja[:10], trainen[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507e02c2-4ec6-4105-b6d2-0e39f1e2912d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:28.112375Z",
     "iopub.status.busy": "2022-06-28T10:59:28.111571Z",
     "iopub.status.idle": "2022-06-28T10:59:28.118103Z",
     "shell.execute_reply": "2022-06-28T10:59:28.117445Z",
     "shell.execute_reply.started": "2022-06-28T10:59:28.112347Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PAD_IDX = ja_vocab['<pad>']\n",
    "BOS_IDX = ja_vocab['<bos>']\n",
    "EOS_IDX = ja_vocab['<eos>']\n",
    "def generate_batch(data_batch):\n",
    "    ja_batch, en_batch = [], []\n",
    "    for (ja_item, en_item) in data_batch:\n",
    "        ja_batch.append(torch.cat([torch.tensor([BOS_IDX]), ja_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    ja_batch = pad_sequence(ja_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return ja_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc30ad10-5b94-41d8-8d04-2089db96a811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:59:28.119129Z",
     "iopub.status.busy": "2022-06-28T10:59:28.118968Z",
     "iopub.status.idle": "2022-06-28T10:59:28.123716Z",
     "shell.execute_reply": "2022-06-28T10:59:28.122935Z",
     "shell.execute_reply.started": "2022-06-28T10:59:28.119112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(PAD_IDX, BOS_IDX, EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5b1591-155b-4e3c-b38f-e46cb25bea84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:32:12.413165Z",
     "iopub.status.busy": "2022-06-28T10:32:12.412932Z",
     "iopub.status.idle": "2022-06-28T10:32:12.430720Z",
     "shell.execute_reply": "2022-06-28T10:32:12.429714Z",
     "shell.execute_reply.started": "2022-06-28T10:32:12.413141Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding +\n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6100e62f-7d7d-4f9f-bef0-c0e12989dcc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:33:55.809633Z",
     "iopub.status.busy": "2022-06-28T10:33:55.809388Z",
     "iopub.status.idle": "2022-06-28T10:33:56.039919Z",
     "shell.execute_reply": "2022-06-28T10:33:56.039046Z",
     "shell.execute_reply.started": "2022-06-28T10:33:55.809604Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(ja_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512 # 512\n",
    "NHEAD = 8 # 8\n",
    "FFN_HID_DIM = 512 # 512\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 3 # 3\n",
    "NUM_DECODER_LAYERS = 3 # 3\n",
    "NUM_EPOCHS = 64\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "def train_epoch(model, train_iter, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in enumerate(train_iter):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                  src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(train_iter)\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                  src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(val_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4bf40df-65c1-446b-954e-72a82c65a376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:34:00.319576Z",
     "iopub.status.busy": "2022-06-28T10:34:00.319161Z",
     "iopub.status.idle": "2022-06-28T10:35:24.609899Z",
     "shell.execute_reply": "2022-06-28T10:35:24.609216Z",
     "shell.execute_reply.started": "2022-06-28T10:34:00.319547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.340, Epoch time = 1.279s\n",
      "Epoch: 2, Train loss: 4.505, Epoch time = 1.306s\n",
      "Epoch: 3, Train loss: 3.914, Epoch time = 1.430s\n",
      "Epoch: 4, Train loss: 3.477, Epoch time = 1.365s\n",
      "Epoch: 5, Train loss: 3.151, Epoch time = 1.307s\n",
      "Epoch: 6, Train loss: 2.864, Epoch time = 1.322s\n",
      "Epoch: 7, Train loss: 2.634, Epoch time = 1.216s\n",
      "Epoch: 8, Train loss: 2.404, Epoch time = 1.342s\n",
      "Epoch: 9, Train loss: 2.244, Epoch time = 1.263s\n",
      "Epoch: 10, Train loss: 2.094, Epoch time = 1.265s\n",
      "Epoch: 11, Train loss: 1.948, Epoch time = 1.323s\n",
      "Epoch: 12, Train loss: 1.836, Epoch time = 1.269s\n",
      "Epoch: 13, Train loss: 1.716, Epoch time = 1.304s\n",
      "Epoch: 14, Train loss: 1.609, Epoch time = 1.244s\n",
      "Epoch: 15, Train loss: 1.525, Epoch time = 1.276s\n",
      "Epoch: 16, Train loss: 1.423, Epoch time = 1.369s\n",
      "Epoch: 17, Train loss: 1.334, Epoch time = 1.398s\n",
      "Epoch: 18, Train loss: 1.252, Epoch time = 1.382s\n",
      "Epoch: 19, Train loss: 1.173, Epoch time = 1.345s\n",
      "Epoch: 20, Train loss: 1.090, Epoch time = 1.309s\n",
      "Epoch: 21, Train loss: 1.028, Epoch time = 1.252s\n",
      "Epoch: 22, Train loss: 0.954, Epoch time = 1.249s\n",
      "Epoch: 23, Train loss: 0.895, Epoch time = 1.212s\n",
      "Epoch: 24, Train loss: 0.836, Epoch time = 1.360s\n",
      "Epoch: 25, Train loss: 0.780, Epoch time = 1.305s\n",
      "Epoch: 26, Train loss: 0.720, Epoch time = 1.330s\n",
      "Epoch: 27, Train loss: 0.676, Epoch time = 1.300s\n",
      "Epoch: 28, Train loss: 0.636, Epoch time = 1.289s\n",
      "Epoch: 29, Train loss: 0.590, Epoch time = 1.309s\n",
      "Epoch: 30, Train loss: 0.541, Epoch time = 1.244s\n",
      "Epoch: 31, Train loss: 0.501, Epoch time = 1.256s\n",
      "Epoch: 32, Train loss: 0.467, Epoch time = 1.238s\n",
      "Epoch: 33, Train loss: 0.424, Epoch time = 1.421s\n",
      "Epoch: 34, Train loss: 0.405, Epoch time = 1.311s\n",
      "Epoch: 35, Train loss: 0.374, Epoch time = 1.223s\n",
      "Epoch: 36, Train loss: 0.340, Epoch time = 1.300s\n",
      "Epoch: 37, Train loss: 0.315, Epoch time = 1.315s\n",
      "Epoch: 38, Train loss: 0.289, Epoch time = 1.291s\n",
      "Epoch: 39, Train loss: 0.272, Epoch time = 1.276s\n",
      "Epoch: 40, Train loss: 0.251, Epoch time = 1.477s\n",
      "Epoch: 41, Train loss: 0.237, Epoch time = 1.398s\n",
      "Epoch: 42, Train loss: 0.218, Epoch time = 1.331s\n",
      "Epoch: 43, Train loss: 0.200, Epoch time = 1.399s\n",
      "Epoch: 44, Train loss: 0.189, Epoch time = 1.367s\n",
      "Epoch: 45, Train loss: 0.176, Epoch time = 1.377s\n",
      "Epoch: 46, Train loss: 0.164, Epoch time = 1.296s\n",
      "Epoch: 47, Train loss: 0.154, Epoch time = 1.377s\n",
      "Epoch: 48, Train loss: 0.142, Epoch time = 1.292s\n",
      "Epoch: 49, Train loss: 0.133, Epoch time = 1.297s\n",
      "Epoch: 50, Train loss: 0.126, Epoch time = 1.225s\n",
      "Epoch: 51, Train loss: 0.116, Epoch time = 1.296s\n",
      "Epoch: 52, Train loss: 0.116, Epoch time = 1.216s\n",
      "Epoch: 53, Train loss: 0.111, Epoch time = 1.309s\n",
      "Epoch: 54, Train loss: 0.101, Epoch time = 1.272s\n",
      "Epoch: 55, Train loss: 0.100, Epoch time = 1.527s\n",
      "Epoch: 56, Train loss: 0.098, Epoch time = 1.418s\n",
      "Epoch: 57, Train loss: 0.088, Epoch time = 1.544s\n",
      "Epoch: 58, Train loss: 0.086, Epoch time = 1.182s\n",
      "Epoch: 59, Train loss: 0.083, Epoch time = 1.402s\n",
      "Epoch: 60, Train loss: 0.083, Epoch time = 1.269s\n",
      "Epoch: 61, Train loss: 0.080, Epoch time = 1.332s\n",
      "Epoch: 62, Train loss: 0.079, Epoch time = 1.328s\n",
      "Epoch: 63, Train loss: 0.076, Epoch time = 1.258s\n",
      "Epoch: 64, Train loss: 0.076, Epoch time = 1.287s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    #print('starting...')\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "    end_time = time.time()\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "317faa63-fea1-428d-9151-4508523474be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:37:28.671831Z",
     "iopub.status.busy": "2022-06-28T10:37:28.671503Z",
     "iopub.status.idle": "2022-06-28T10:37:32.033185Z",
     "shell.execute_reply": "2022-06-28T10:37:32.032221Z",
     "shell.execute_reply.started": "2022-06-28T10:37:28.671803Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "time_s = now.strftime(\"%H_%M_%S\")\n",
    "\n",
    "#torch.save(transformer, str('model' + str(time_s) + '.model'))\n",
    "torch.save(transformer, 'model_production.model')\n",
    "\n",
    "def dump_var(var):\n",
    "    var_name = f'{var=}'.split('=')[0]\n",
    "    with open(str(var_name + str(time_s) + '.pickle'), 'wb') as handle: pickle.dump(var, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def dump_last(var):\n",
    "    var_name = f'{var=}'.split('=')[0]\n",
    "    with open(str(var_name + '.pickle'), 'wb') as handle: pickle.dump(var, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#dump_var(ja_vocab)\n",
    "#dump_var(en_vocab)\n",
    "#\n",
    "#dump_last(ja_vocab)\n",
    "#dump_last(en_vocab)\n",
    "#\n",
    "with open('ja_vocab_production.pickle', 'wb') as handle: pickle.dump(ja_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('en_vocab_production.pickle', 'wb') as handle: pickle.dump(en_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tokenizer_production.pickle', 'wb') as handle: pickle.dump(ja_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db33f5c-6560-448c-a865-c9f07fd83c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:35:37.256830Z",
     "iopub.status.busy": "2022-06-28T10:35:37.256573Z",
     "iopub.status.idle": "2022-06-28T10:35:37.267442Z",
     "shell.execute_reply": "2022-06-28T10:35:37.265908Z",
     "shell.execute_reply.started": "2022-06-28T10:35:37.256804Z"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(device)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX: break\n",
    "    return ys\n",
    "\n",
    "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = [BOS_IDX] + [src_vocab.get_stoi().get(tok,0) for tok in src_tokenizer.encode(src, out_type=str)]+ [EOS_IDX]\n",
    "    num_tokens = len(tokens)\n",
    "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join([tgt_vocab.get_itos()[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cd773b3-af91-440e-8b09-f5c9e146d748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T10:37:52.584076Z",
     "iopub.status.busy": "2022-06-28T10:37:52.583802Z",
     "iopub.status.idle": "2022-06-28T10:37:53.122077Z",
     "shell.execute_reply": "2022-06-28T10:37:53.121298Z",
     "shell.execute_reply.started": "2022-06-28T10:37:52.584037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$ y + y $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ \\lim _ {x\\to a} f (x) =L, $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ \\sin a\\sin b $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ \\cos {2}  $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ \\cos {x}  $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ log_ {x}  {x}  $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$ log_ {ψ}  {ψ}  $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Latex\n",
    "\n",
    "transformer2 = torch.load('model_production.model')\n",
    "\n",
    "with open('ja_vocab_production.pickle', 'rb') as handle:\n",
    "    ja_vocab2 = pickle.load(handle)\n",
    "    \n",
    "with open('en_vocab_production.pickle', 'rb') as handle:\n",
    "    en_vocab2 = pickle.load(handle)\n",
    "    \n",
    "with open('tokenizer_production.pickle', 'rb') as handle:\n",
    "    ja_tokenizer2 = pickle.load(handle)\n",
    "         \n",
    "def tr(word):\n",
    "    eq = translate(transformer2, word, ja_vocab2, en_vocab2, ja_tokenizer2)\n",
    "    display(Latex(f'${eq}$'))\n",
    "    return eq\n",
    "\n",
    "tests_ = [\"игрек плюс икс\", 'предел эф от икс при икс стремящемся к альфа',\n",
    "         'синус альфа синус бета', 'косинус двух', 'косинус икс', 'логарифм косинуса икс по основанию три','логарифм альфа по основанию пси']\n",
    "res = ['x+y', '\\lim f(x)', '\\sin \\alpha', '\\cos 2']\n",
    "#mean_bleu = 0\n",
    "for key,test_ in enumerate(tests_):\n",
    "    tr(test_)\n",
    "#print(mean_bleu/len(tests_))\n",
    "\n",
    "#test_ = 'синус игрек'\n",
    "#tr(test_)\n",
    "\n",
    "### эти лучшие тесты были на большом датасете из маленьких кусков на простой токенизации и вроде 64 эпохах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d55ac6-7ac5-4d38-a9d2-0d8eb5f718f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer, ja_vocab, en_vocab, ja_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead53748-6f86-4124-8b2c-5c5eccddc65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T09:32:07.092683Z",
     "iopub.status.busy": "2022-06-28T09:32:07.092393Z",
     "iopub.status.idle": "2022-06-28T09:32:07.097096Z",
     "shell.execute_reply": "2022-06-28T09:32:07.096249Z",
     "shell.execute_reply.started": "2022-06-28T09:32:07.092656Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install jiwer\n",
    "import nltk\n",
    "\n",
    "#from jiwer import wer\n",
    "#from jiwer import cer\n",
    "#\n",
    "#gleu = 0\n",
    "#cer_error = 0\n",
    "#error = 0\n",
    "#\n",
    "##bleu = 0\n",
    "#tests = test.values.tolist()\n",
    "#\n",
    "#for sample in tests:\n",
    "#    rus = sample[0]\n",
    "#    true = sample[1]\n",
    "#    sugg = translate(transformer, rus, ja_vocab, en_vocab, ja_tokenizer)\n",
    "#    \n",
    "#    #bleu_sample = nltk.translate.bleu_score.sentence_bleu([sugg], true, weights=(0.5, 0.25, 0.10, 0.05))\n",
    "#    gleu_sample = nltk.translate.gleu_score.sentence_gleu([simple_latex_splitter(sugg)], simple_latex_splitter(true))\n",
    "#    \n",
    "#    #bleu += bleu_sample\n",
    "#    gleu += gleu_sample\n",
    "#    \n",
    "#    ground_truth = true\n",
    "#    hypothesis = sugg\n",
    "#    \n",
    "#    error += wer(ground_truth, hypothesis)\n",
    "#    cer_error += cer(ground_truth, hypothesis)\n",
    "#    \n",
    "##bleu = bleu / len(tests)\n",
    "#gleu = gleu / len(tests)\n",
    "#error = error / len(tests)\n",
    "#cer_error = cer_error / len(tests)\n",
    "#print(gleu, error, cer_error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
